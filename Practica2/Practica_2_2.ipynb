{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2\n",
    "## Parte 2. Recuperación de Información\n",
    "<br>\n",
    "\n",
    "__Alumnos:__\n",
    "* __Frederick Ernesto Borges Noronha__\n",
    "* __Victor Manuel Cavero Gracia__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí vamos a utilizar el conjunto de datos de 20 Newsgroups que se encuentra disponible en Scikit-learn y que hemos usado en el notebook de ejemplo.\n",
    "\n",
    "El conjunto consiste en textos de un foro sobre diferentes temas, desde hardware hasta religión. Algunos temas están muy relacionados, p.ej. \"IBM PC hardware\" y \"Mac hardware\", mientras que otros son más diversos, p.ej. \"religion\" o \"hockey\").\n",
    "\n",
    "El objetivo de esta parte es poner en poner en práctica los conceptos de recuperación de información para realizar un buscador de mensajes en un foro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apartado A\n",
    "\n",
    "El conjunto está dividido de forma predeterminada en entrenamiento y prueba en porcentajes de 60 y 40, respectivamente (como se puede ver en el notebook de prueba). Usaremos únicamente la parte de entrenamiento como los mensajes a recuperar por nuestro buscador.\n",
    "\n",
    "Vamos a utilizar una representación de la bolsa de palabras de **countVectorizer** con las siguientes opciones:\n",
    "- La bolsa de palabras tendrá en cuenta la frecuencia de las palabras en cada mensaje (**binary=False**)\n",
    "- Usa el diccionario que se encuentra en la siguiente URL y que ya usamos en el notebook de prueba. https://github.com/dwyl/english-words/blob/master/words.txt\n",
    "- Usa la lista de palabras vacías (parámetro **stop_words**) que proporciona sklearn para el inglés\n",
    "- Usando un rango de n-gramas de (1,1) (parámetro **ngram_range**).\n",
    "\n",
    "Para calcular la similitud entre dos mensajes usaremos la similitud del coseno (**sklearn.metrics.pairwise.cosine_similarity**) que es capaz de medir la similitud entre los elementos (es decir, entre las filas) de dos matrices de vectores de términos pudiendo ser estas matrices densas o dispersas.\n",
    "\n",
    "Toma 3 mensaje del conjunto de prueba para cada clase (es decir, para cada tema). Vas a usar cada uno de dichos mensajes como consulta para recuperar los mensajes del conjunto de entrenamiento que más se parezcan a la consulta. Para ello sigue los siguientes pasos:\n",
    "\n",
    "1. Usa la distancia del coseno entre el mensaje de consulta y los mensajes de entrenamiento.\n",
    "\n",
    "2. Ordena los resultados de mayor a menor relevancia con la consulta.\n",
    "\n",
    "3. Calcula la precisión de la lista de resultados con nivel de exhaustividad 3 y 10. \n",
    "\n",
    "    - La precisión a un nivel de exhaustividad X es el número de resultados que son relevantes (es decir, de la clase buscada) de entre los X primeros recuperados.\n",
    "\n",
    "4. Calcula los valores de precisión media (para cada nivel de exhaustividad) para cada clase del conjunto de datos.\n",
    "\n",
    "Se valorará el uso de funciones y la claridad del código, así como sus comentarios. Contesta a lo siguiente:\n",
    "\n",
    "- ¿Hay muchas diferencias entre los valores de precisión medios para las distintas clases del conjunto de datos? ¿A qué crees que se deben?\n",
    "\n",
    "- Identifica la clase que haya tenido peores resultados de precisión y para alguna de sus consultas muestra alguno de los mensajes que recuperó erróneamente en las primeras X posiciones.\n",
    "\n",
    "    - ¿Con qué clases se ha confundido más dicha consulta? \n",
    "\n",
    "    - ¿A qué crees que se deben los malos resultados?\n",
    "\n",
    "Debes usar la parte de entrenamiento para construir la bolsa de palabras con frecuencia y bolsa de palabras con TF/IDF.\n",
    "\n",
    "### Apartado B\n",
    "\n",
    "Repite la secuencia de pasos descritos en el apartado a) pero ahora usa TF-IDF para ponderar el peso de los términos de la bolsa de palabras.  Para usar TF-IDF primero debes transformar los textos  usando  **countVectorizer** con **binary=False**  para  obtener  la  frecuencia  de  palabras  (exactamente igual que en el apartado anterior), y a continuación usar **TfidfTransformer** para modular dicha frecuencia según lo popular que sea cada término en el conjunto de mensajes de entrenamiento.\n",
    "\n",
    "A continuación contesta a lo siguiente.\n",
    "\n",
    "- ¿Han cambiado los valores de precisión media para las clases del conjunto de datos?¿Qué clases han mejorado? ¿Cuáles han empeorado?\n",
    "\n",
    "- Encuentra una consulta donde el uso de la ponderación TF-IDF haya sido efectivo y haya mejorado los resultados. Explica por qué ha sido efectivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports:\n",
    "A continuación se encuentran todos los imports de las librerias de las que haremos uso en esta práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training texts: 11314\n",
      "Test texts: 7532\n"
     ]
    }
   ],
   "source": [
    "train_data = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "print(\"Training texts:\", len(train_data.data))\n",
    "print(\"Test texts:\", len(test_data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Length:  466551  words\n"
     ]
    }
   ],
   "source": [
    "# Crear el diccionario de palabras\n",
    "with open('Datos/words.txt') as f:\n",
    "    dictionary = f.read().splitlines()\n",
    "\n",
    "print(\"Dictionary Length: \", len(dictionary), \" words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(vocabulary=dictionary, stop_words='english', ngram_range=(1,1), binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bitfdce462a73ae44f1b321382e2283b5f2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
